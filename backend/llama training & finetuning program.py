# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g59DUkPFmSlSmMG8Fou59b8c3sAXOeLo
"""

!pip install --upgrade transformers datasets sentence-transformers spacy PyMuPDF torch nltk accelerate bitsandbytes
!python -m spacy download en_core_web_sm

# ===============================
# 1. Environment Setup & Mounting Google Drive
# ===============================
from google.colab import drive
drive.mount('/content/drive')

import os
import json
import nltk
import torch
import pickle
import numpy as np
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from nltk.tokenize import word_tokenize

nltk.download('punkt')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… Using device:", device)

# ===============================
# 2. Authentication for Hugging Face
# ===============================
# ðŸ” Replace this token with your actual Hugging Face token
HUGGINGFACE_TOKEN = "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"  # â¬…ï¸ REPLACE THIS
os.environ["HF_ACCESS_TOKEN"] = HUGGINGFACE_TOKEN

# ===============================
# 3. Load and Analyze the MANISH.json Dataset
# ===============================
dataset_path = "/content/drive/My Drive/MANISH.json"
with open(dataset_path, 'r', encoding='utf-8') as f:
    raw_data = json.load(f)
print(f"âœ… Loaded {len(raw_data)} cases from MANISH.json.")

def create_training_text(case: dict) -> str:
    # Gather all parameters from the dataset; if a parameter is missing, use a default.
    case_id = case.get("case_id", "UNKNOWN")
    summary = case.get("summary", "")
    questions = " ".join(case.get("questions", []))
    strategic_advice = " ".join(case.get("strategic_advice", []))
    estimated_outcome = case.get("estimated_outcome", "")
    relevant_sections = " ".join(case.get("relevant_sections", []))
    suggested_actions = " ".join(case.get("suggested_actions", []))

    # Concatenate all information into a single training text.
    full_text = (
        f"CASE_ID: {case_id}\n"
        f"Summary: {summary}\n"
        f"Clarifying Questions: {questions}\n"
        f"Strategic Advice: {strategic_advice}\n"
        f"Estimated Outcome: {estimated_outcome}\n"
        f"Relevant Legal Sections: {relevant_sections}\n"
        f"Suggested Actions: {suggested_actions}\n"
    )
    return full_text.strip()

training_texts = [create_training_text(case) for case in raw_data if case.get("summary")]
print("âœ… Created training texts for", len(training_texts), "cases.")
print("ðŸ“Œ Sample training text (first 500 characters):\n", training_texts[0][:500])
print("-" * 80)

# ===============================
# 4. Create Hugging Face Dataset from Training Texts
# ===============================
hf_dataset = Dataset.from_dict({"text": training_texts})
print("âœ… Hugging Face dataset created with", len(hf_dataset), "examples.")

# ===============================
# 5. Tokenization with Maximum Context Length
# ===============================
# Set maximum context length (adjust if needed; many LLaMA models support 4096 tokens)
max_context_length = 4096

model_id = "meta-llama/Llama-3.2-1B"
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)
tokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS token

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=max_context_length)

tokenized_dataset = hf_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
print("âœ… Tokenization complete. Using max_length =", max_context_length)

# ===============================
# 6. Load the Generative Model for Fine-Tuning
# ===============================
model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=True)
model.to(device)
print("âœ… Loaded model:", model_id)

# ===============================
# 7. Setup Training Arguments and Trainer
# ===============================
training_args = TrainingArguments(
    output_dir="/content/drive/My Drive/llama_finetuned",
    num_train_epochs=1,                        # Adjust epochs as needed
    per_device_train_batch_size=1,             # Lower batch size due to long sequence length
    learning_rate=5e-5,
    logging_steps=100,
    save_strategy="epoch",
    fp16=True if torch.cuda.is_available() else False,
    overwrite_output_dir=True,
    report_to="none"
)

def data_collator(features: list) -> dict:
    """
    Data collator that will dynamically pad the inputs received,
    as well as the labels.
    """
    # As the examples are generated on the fly, we need to pad them dynamically
    # with a data collator.
    # For this, we will use the default data collator provided by the transformers library.
    # We will modify the labels by shifting the input_ids by one position to the right.
    # This is necessary for causal language modeling tasks where the input_ids
    # themselves serve as the labels.
    from transformers import DataCollatorForLanguageModeling
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    # MLM is set to False as we are not doing masked language modeling.
    # We are doing causal language modeling.
    batch = data_collator(features)
    # This will pad the inputs and labels dynamically
    # with the default data collator.
    # We will now modify the labels to be the same as the inputs
    # but shifted by one position to the right.
    # This is necessary for causal language modeling tasks.
    batch["labels"] = batch["input_ids"].clone()
    return batch


# IMPORTANT: Removed the "tokenizer" parameter to avoid the unexpected "compute_loss" error.
# Added the data_collator parameter to dynamically pad the inputs and labels.
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

print("ðŸš€ Starting fine-tuning...")
trainer.train()
print("âœ… Fine-tuning completed.")

# ===============================
# 8. Save the Fine-Tuned Model as a .pkl File
# ===============================
save_path = "/content/llama_finetuned_state_dict.pkl"
with open(save_path, "wb") as f:
    pickle.dump(model.state_dict(), f)
print(f"ðŸ’¾ Model state_dict saved as .pkl at: {save_path}")

# ===============================
# 9. Inference Example Using Maximum Context Length
# ===============================
def generate_response(prompt: str, max_length=4069) -> str:
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=max_context_length).to(device)
    with torch.no_grad():
        output_ids = model.generate(
            inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=max_length,
            num_beams=4,
            repetition_penalty=2.5,
            no_repeat_ngram_size=3,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=False
        )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

sample_prompt = (
    "User Query: I signed a contract with a vendor to supply materials for my construction project. "
    "The materials were delivered two months late. Can I claim damages for the delay?\n\n"
    "Answer in a legal tone:"
)
response = generate_response(sample_prompt, max_length=4096)
print("\nðŸ§  Generated Response from Fine-Tuned Model:\n", response)

# Load tokenizer and architecture
model = AutoModelForCausalLM.from_pretrained(model_id, token=HUGGINGFACE_TOKEN)
with open("/content/drive/My Drive/llama_finetuned_state_dict.pkl", "rb") as f:
    model.load_state_dict(pickle.load(f))
model.to(device)

# ===============================
# 9. Interactive Two-Phase Inference Pipeline with Refined Prompts
# ===============================

# --- Phase 1: Generate Clarifying Questions ---
def build_prompt_part1(query: str, case_summary: str) -> str:
    # Provide a few-shot example to guide the generation of questions.
    few_shot_example = (
        "Example:\n"
        "1. When was the contract signed, and was it documented in writing?\n"
        "2. Does the contract specify any delivery deadlines or conditions for delay penalties?\n"
        "3. Can you provide evidence of the agreed delivery date and the actual delivery date?\n"
        "4. Have you communicated with the vendor regarding the delay and its impact on your project?\n\n"
    )
    prompt = (
        "You are a legal assistant specializing in contract law.\n\n"
        "Generate exactly 4 detailed clarifying questions to better understand the user's situation. "
        "Your response should ONLY include the 4 questions, each on a new line, and end with the delimiter '---PART 1 END---'.\n\n"
        "User Query:\n" + query + "\n\n"
        "Case Summary:\n" + case_summary + "\n\n"
        + few_shot_example +
        "Questions:\n"
    )
    return prompt

# --- Phase 2: Generate Final Structured Response ---
def build_prompt_part2(query: str, case_summary: str, clarifying_answers: str) -> str:
    # Few-shot example for PART 2 output
    few_shot_example = (
        "Example Output Format:\n\n"
        "PART 2:\n"
        "a) Relevant Legal Sections and Articles: Section 73 and Section 74 of the Indian Contract Act, 1872 apply in cases of breach of contract.\n"
        "b) Suggested Legal Procedures: Gather evidence, send a legal notice, and consider mediation or litigation.\n"
        "c) Strategic Advice: Document all communications, consult a legal expert, and explore dispute resolution options.\n"
        "d) Estimated Outcome: The court may award compensatory damages based on proven financial losses.\n\n"
    )
    prompt = (
        "You are a legal assistant specializing in contract law. Based on the information below, generate the final legal analysis.\n\n"
        "Your response MUST include the following sections exactly as labeled:\n"
        "   a) Relevant Legal Sections and Articles\n"
        "   b) Suggested Legal Procedures\n"
        "   c) Strategic Advice\n"
        "   d) Estimated Outcome\n\n"
        "Follow the format in the example exactly.\n\n"
        + few_shot_example +
        "Now, generate your response using the information provided below.\n\n"
        "User Query:\n" + query + "\n\n"
        "Case Summary:\n" + case_summary + "\n\n"
        "User Answers to Clarifying Questions:\n" + clarifying_answers + "\n\n"
        "PART 2 START:\n"
    )
    return prompt

def generate_text(prompt: str, max_length: int = 2048) -> str:
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=max_context_length).to(device)
    with torch.no_grad():
        output_ids = model.generate(
            inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=max_length,
            num_beams=4,
            repetition_penalty=2.5,
            no_repeat_ngram_size=3,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=False
        )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

def retrieve_case_summary(query: str) -> str:
    # Optionally, use retrieval to get a case summary from the dataset.
    # For this example, we simply use the summary from the first case.
    if len(raw_data) > 0:
        return raw_data[0].get("summary", "")
    return ""

# --- Interactive Two-Phase Inference ---

# Phase 1: Clarifying Questions Generation
sample_query = (
    "I signed a contract with a vendor to supply materials for my construction project. "
    "We agreed on a delivery date, but they delivered the materials two months late, causing significant delays. "
    "The contract doesn't specifically mention penalties for late delivery. Can I claim damages for the losses I suffered due to the delay?"
)
case_summary = retrieve_case_summary(sample_query)
prompt_part1 = build_prompt_part1(sample_query, case_summary)
print("Composite Prompt for PART 1:\n", prompt_part1)

# Generate PART 1 output
part1_response = generate_text(prompt_part1, max_length=64000)
# Extract text until the delimiter is found
if "---PART 1 END---" in part1_response:
    clarifying_questions = part1_response.split("---PART 1 END---")[0].strip()
else:
    clarifying_questions = part1_response.strip()

print("\n--- PART 1 Output (Clarifying Questions) ---\n")
print(clarifying_questions)

# Phase 2: Final Structured Response
print("\nPlease provide your answers to the clarifying questions (one per line):")
user_answers = []
for i in range(4):
    ans = input(f"Answer to question {i+1}: ")
    user_answers.append(ans)
clarifying_answers = "\n".join(user_answers)

prompt_part2 = build_prompt_part2(sample_query, case_summary, clarifying_answers)
print("\nComposite Prompt for PART 2:\n", prompt_part2)

# Generate PART 2 output
part2_response = generate_text(prompt_part2, max_length=64000)
final_structured_response = part2_response.strip()
print("\n--- PART 2 Output (Final Structured Response) ---\n")
print(final_structured_response)